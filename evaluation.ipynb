{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096fcf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import scipy.spatial # very important, does not work without it, i don't know why\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "random.seed(0)\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # to avoid warnings\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e82de8",
   "metadata": {},
   "source": [
    "## RDKIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4033970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_fp(smiles: str):\n",
    "    fp_obj = AllChem.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), radius=2, nBits=2048,\n",
    "                                                   useChirality=False)\n",
    "    return fp_obj\n",
    "\n",
    "def get_fp_np(smiles: str):\n",
    "    fp_obj = get_fp(smiles)\n",
    "    fp = np.zeros((0,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(fp_obj, fp)\n",
    "    return fp\n",
    "\n",
    "\n",
    "def read_csv(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    non_chiral_smiles = df.iloc[:, 1].tolist()\n",
    "    backbones = df.iloc[:, 2].tolist()\n",
    "    chains = df.iloc[:, 3].tolist()\n",
    "    assay_ids = df.iloc[:, 4].tolist()\n",
    "    types = df.iloc[:, 5].tolist()\n",
    "    labels = df.iloc[:, 6].tolist()\n",
    "    return non_chiral_smiles, backbones, chains, assay_ids, types, labels\n",
    "\n",
    "\n",
    "def calc_tani_sim(mol1_smiles, mol2_smiles):\n",
    "    mol1 = Chem.MolFromSmiles(mol1_smiles)\n",
    "    mol2 = Chem.MolFromSmiles(mol2_smiles)\n",
    "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, radius=2, nBits=2048, useChirality=False)\n",
    "    fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, radius=2, nBits=2048, useChirality=False)\n",
    "    tani_sim = DataStructs.FingerprintSimilarity(fp1, fp2, metric=DataStructs.TanimotoSimilarity)\n",
    "    return tani_sim\n",
    "\n",
    "\n",
    "def get_canonical_smiles(smiles, chirality=True):\n",
    "    chiral = 1 if chirality else 0\n",
    "    return Chem.CanonSmiles(smiles, useChiral=chiral)\n",
    "\n",
    "\n",
    "def load_tokenizer_from_file(file_path: str):\n",
    "    return Tokenizer.from_file(file_path)\n",
    "\n",
    "\n",
    "def smiles_valid(smiles,verbose=False):\n",
    "    if smiles is None:\n",
    "        return False\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        return True\n",
    "    print(smiles)\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(lst, n):\n",
    "    \"\"\"Split list into n chunks as evenly as possible.\"\"\"\n",
    "    k, m = divmod(len(lst), n)\n",
    "    return [lst[i*k + min(i, m):(i+1)*k + min(i+1, m)] for i in range(n)]\n",
    "\n",
    "def create_cross_splits(lst, num_chunks=10):\n",
    "    \"\"\"Create cross splits: each split is (all_but_one_chunk, one_chunk).\"\"\"\n",
    "    lst = list(lst)\n",
    "    random.shuffle(lst)\n",
    "    \n",
    "    chunks = split_into_chunks(lst, num_chunks)\n",
    "    splits = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        held_out = chunks[i]\n",
    "        rest = [item for j, chunk in enumerate(chunks) if j != i for item in chunk]\n",
    "        splits.append((rest, held_out))\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = torch.get_num_threads()\n",
    "\n",
    "def get_many_fps(smiles):\n",
    "    with torch.multiprocessing.Pool(cpus) as pool:\n",
    "        fps = pool.map(get_fp_np, smiles)\n",
    "    return fps\n",
    "\n",
    "def get_sims(args):\n",
    "    org, opt = args\n",
    "    fps = list(map(get_fp, [org, opt]))\n",
    "    org_fp, opt_fp = fps[0], fps[1]\n",
    "    sims = DataStructs.FingerprintSimilarity(org_fp, opt_fp, metric=DataStructs.TanimotoSimilarity)\n",
    "    return sims\n",
    "\n",
    "def get_many_sims(smiles_lists):\n",
    "    with torch.multiprocessing.Pool(cpus) as pool:\n",
    "        sims = pool.map(get_sims, smiles_lists)\n",
    "    return sims    \n",
    "\n",
    "def get_many_canons(smiles_lists):\n",
    "    smiles = sum(smiles_lists, [])\n",
    "    with torch.multiprocessing.Pool(cpus) as pool:\n",
    "        canons = pool.map(get_canonical_smiles, smiles)\n",
    "    canons_lists = []\n",
    "    for l in smiles_lists:\n",
    "        canons_lists.append(canons[:len(l)])\n",
    "        canons = canons[len(l):]\n",
    "    return canons\n",
    "\n",
    "\n",
    "def get_clf(positive_smiles, negative_smiles, test_fraction=0.2, num_estimators=100, max_depth=2):\n",
    "    positive_fp = get_many_fps(positive_smiles)\n",
    "    negative_fp = get_many_fps(negative_smiles)\n",
    "    # random.shuffle(positive_fp)\n",
    "    # random.shuffle(negative_smiles)\n",
    "    \n",
    "    pos_ratio = len(positive_fp) / (len(negative_fp) + len(positive_fp))\n",
    "    num_test = int(test_fraction * (len(negative_fp) + len(positive_fp)))\n",
    "    pos_test_fp = positive_fp[:int(pos_ratio*num_test)]\n",
    "    pos_train_fp = positive_fp[int(pos_ratio * num_test):]\n",
    "    neg_test_fp = negative_fp[:int((1-pos_ratio)*num_test)]\n",
    "    neg_train_fp = negative_fp[int((1-pos_ratio) * num_test):]\n",
    "    X = np.stack(pos_train_fp + neg_train_fp, axis=0)\n",
    "    y = np.concatenate([np.ones(len(pos_train_fp)), np.zeros(len(neg_train_fp))])\n",
    "    sample_weights = [1 if cur_label == 0 else len(neg_train_fp) / len(pos_train_fp) for cur_label in y]\n",
    "    clf = RandomForestClassifier(max_depth=max_depth, random_state=0, n_estimators=num_estimators)\n",
    "    clf.fit(X, y, sample_weight=sample_weights)\n",
    "    positives = np.ones(len(pos_test_fp))\n",
    "    negatives = np.zeros(len(neg_test_fp))\n",
    "    labels = np.concatenate([positives, negatives], axis=0)\n",
    "    test_samples = np.concatenate([pos_test_fp, neg_test_fp], axis=0)\n",
    "    probs = clf.predict_proba(test_samples)\n",
    "    \n",
    "    roc_auc = roc_auc_score(labels, probs[:, 1])\n",
    "    fpr, tpr, thresholds = roc_curve(labels, probs[:, 1], pos_label=1)\n",
    "    return clf, roc_auc, fpr, tpr, thresholds, thresholds[np.argmax(tpr - fpr)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c7618",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6254557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = {}\n",
    "    for smile, _, _, assay_id, _, label in zip(*read_csv(path)):\n",
    "        if assay_id not in data:\n",
    "            data[assay_id] = {\n",
    "                'active': [],\n",
    "                'inactive': []\n",
    "            }    \n",
    "        if label == 1:\n",
    "            data[assay_id]['active'].append(smile)\n",
    "        else:\n",
    "            data[assay_id]['inactive'].append(smile)\n",
    "    return data\n",
    "\n",
    "def load_clfs(assay_ids, data):\n",
    "    clfs = {}\n",
    "    for name in tqdm(assay_ids):\n",
    "        print(f'-------{name}--------------')\n",
    "        clfs[name] = []\n",
    "        roc_aucs = []\n",
    "        for (active_train, active_test), (inactive_train, inactive_test) in zip(\n",
    "            create_cross_splits(data[name]['active'], 10), \n",
    "            create_cross_splits(data[name]['inactive'], 10)\n",
    "        ):\n",
    "            clf, roc_auc, fpr, tpr, thresholds, best_thresh = get_clf(active_train, inactive_train)\n",
    "            # res = get_clf(data[name]['active'], data[name]['inactive'])\n",
    "            clfs[name].append(dict(clf=clf,thresh=best_thresh,unseen_active=set(active_test), unseen_inactive=set(inactive_test)))\n",
    "            roc_aucs.append(roc_auc)\n",
    "        print('roc:', sum(roc_aucs)/len(roc_aucs))\n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def score_smiles(clf,smiles):\n",
    "    smiles_dct = defaultdict(list)\n",
    "    for i,sml in enumerate(smiles):\n",
    "        smiles_dct[sml].append(i)\n",
    "    smiles_unq = list(smiles_dct.keys())\n",
    "    fps = get_many_fps(smiles_unq)\n",
    "    fps = np.stack(fps, axis=0)\n",
    "    scores = clf.predict_proba(fps)[:,1]\n",
    "    res = [None]*(len(smiles))\n",
    "    for score,sml in zip(scores,smiles_unq):\n",
    "        for i in smiles_dct[sml]:\n",
    "            res[i] = score\n",
    "    return res\n",
    "\n",
    "\n",
    "def sample_from_file(path):\n",
    "    with open(path) as f:\n",
    "        all_pairs = []\n",
    "        for line in f.readlines()[1:]:\n",
    "            if \"None\" in line:\n",
    "                continue\n",
    "            parts = line.replace('\\n', '').replace(',',' ').split(' ')\n",
    "            all_pairs.append((parts[0], parts[1]))\n",
    "    opt_samples = {}\n",
    "    for pair in all_pairs:\n",
    "        if pair[0] in opt_samples:\n",
    "            opt_samples[pair[0]].append(pair[1])\n",
    "            opt_samples[pair[0]] = opt_samples[pair[0]][:20]\n",
    "        else:\n",
    "            opt_samples[pair[0]] = [pair[1]]\n",
    "    return opt_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d099f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_names = [\n",
    "    \"CHEMBL1119333\",\n",
    "    \"CHEMBL1614027\",\n",
    "    \"CHEMBL1614423\",\n",
    "    \"CHEMBL1738485\",\n",
    "    \"CHEMBL1963715\",\n",
    "    \"CHEMBL1963723\",\n",
    "    \"CHEMBL1963731\",\n",
    "    \"CHEMBL1963741\",\n",
    "    \"CHEMBL1963756\",\n",
    "    \"CHEMBL1963810\",\n",
    "    \"CHEMBL1963818\",\n",
    "    \"CHEMBL1963819\",\n",
    "    \"CHEMBL1963824\",\n",
    "    \"CHEMBL1963825\",\n",
    "    \"CHEMBL1963827\",\n",
    "    \"CHEMBL1963831\",\n",
    "    \"CHEMBL1964101\",\n",
    "    \"CHEMBL1964115\",\n",
    "    \"CHEMBL3214944\",\n",
    "    \"CHEMBL3431930\",\n",
    "    \"CHEMBL3431932\",\n",
    "]\n",
    "df = pd.read_csv('./data/splits/test.csv')\n",
    "full_tasks = df[['assay_id','non_chiral_smiles']][df['active']==0]\n",
    "full_tasks = full_tasks[full_tasks['assay_id'].isin(tasks_names)]\n",
    "full_tasks = full_tasks.rename(columns={'assay_id': 'Assay', 'non_chiral_smiles': 'Org'})\n",
    "index = full_tasks.groupby(['Assay','Org']).agg('max').index\n",
    "\n",
    "seen_smls = set(pd.read_csv('./data/splits/train.csv')['non_chiral_smiles'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648afb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= load_data('./data/splits/test.csv')\n",
    "clfs = load_clfs(tasks_names, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ecc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(len(clfs[tasks_names[0]][i]['unseen_inactive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eee0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generated(path):\n",
    "    generated_smiles = {}\n",
    "    for task in tasks_names:\n",
    "        generated_smiles[task] = sample_from_file(path % task)\n",
    "        canon_lists = get_many_canons(list(generated_smiles[task].values()))\n",
    "        for org_mol, canons in zip(generated_smiles[task], canon_lists):\n",
    "            generated_smiles[task][org_mol] = canons\n",
    "    return generated_smiles\n",
    "\n",
    "def score_mols_df(generated_smiles):\n",
    "    columns =['Assay', 'Org', 'Opt', 'Score_Org',  'Thresh_Org', 'Score_Opt','Thresh_Opt', 'Sim']\n",
    "    rows = []\n",
    "    for task in tqdm(tasks_names):\n",
    "        for clf_dict in clfs[task]:\n",
    "            optimized_mols = []\n",
    "            origin_mols = []\n",
    "            for org_mol in clf_dict['unseen_inactive']:\n",
    "                if org_mol not in generated_smiles[task]:\n",
    "                    continue\n",
    "                for opt_mol in generated_smiles[task][org_mol]:\n",
    "                    origin_mols.append(org_mol)\n",
    "                    optimized_mols.append(opt_mol)\n",
    "        # print(f'Assay: {task}')\n",
    "            clf = clf_dict['clf']\n",
    "            thresh = clf_dict['thresh']\n",
    "            scores = score_smiles(clf, origin_mols + optimized_mols)\n",
    "            origin_scores = scores[:len(origin_mols)]\n",
    "            optimized_scores = scores[len(origin_mols):]\n",
    "            \n",
    "            sims = get_many_sims(list(zip(origin_mols, optimized_mols)))\n",
    "            \n",
    "            for org_mol, opt_mol, org_score, opt_score, sim in zip(origin_mols, optimized_mols, origin_scores, optimized_scores, sims):\n",
    "                \n",
    "                rows.append([task, org_mol, opt_mol, org_score, org_score > thresh, opt_score, opt_score > thresh, sim])\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    return df\n",
    "\n",
    "def print_df(df):\n",
    "    print('Index', *df.columns)\n",
    "    for idx, row in zip(df.index, df.values):\n",
    "        print(idx, *['%.3f'%v for v in row], sep=' & ', end='\\\\\\\\\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom, chi2\n",
    "\n",
    "def mcnemar(l1, l2, continuity_correction: bool = False) -> float:\n",
    "    l1 = np.array(l1, dtype=bool)\n",
    "    l2 = np.array(l2, dtype=bool)\n",
    "    b = int(np.sum(l1 & (~l2)))  # count of positives in l1 and negatives in l2\n",
    "    c = int(np.sum((~l1) & l2))  # count of positives in l2 and negatives in l1\n",
    "    \n",
    "    check_valid = lambda n: isinstance(n, int) or (isinstance(n, float) and n.is_integer())\n",
    "    if not all(map(check_valid, [b, c])):\n",
    "        raise ValueError(\"b and c must be integers!\")\n",
    "    n_min, n_max = sorted([b, c])\n",
    "    corr = int(continuity_correction)\n",
    "    if (n_min + n_max) < 25:\n",
    "        pvalue = 2 * binom.cdf(n_min, n_min+n_max, 0.5) - binom.pmf(n_min, n_min+n_max, 0.5)\n",
    "    else:\n",
    "        chi2_statistic = (abs(n_min - n_max) - corr) ** 2 / (n_min + n_max)\n",
    "        pvalue = chi2.sf(chi2_statistic, 1)\n",
    "    return pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtask_agg = 'max' # 'max' or 'mean', depending on how you want to aggregate the success metric\n",
    "\n",
    "def calc_pvalue(succ1, succ2,assay=''):\n",
    "    successes = succ1.join(succ2, lsuffix='_1', rsuffix='_2', how='inner')\n",
    "    agg_results = {}\n",
    "    for cur_assay, group in successes.groupby(level='Assay'):\n",
    "        \n",
    "        if assay and cur_assay != assay:\n",
    "            continue\n",
    "        l1 = group['Success_1'].values\n",
    "        l2 = group['Success_2'].values\n",
    "        pv = mcnemar(l1 ,l2, True)\n",
    "        # pv = scipy.stats.ttest_rel(l1,l2).pvalue\n",
    "        agg_results[cur_assay] = pv\n",
    "    agg_df = pd.DataFrame.from_dict(agg_results, orient='index', columns=['pvalue'])  \n",
    "    return agg_df\n",
    "\n",
    "sim= 0.4\n",
    "def calc_success(df):\n",
    "    df['Success'] = df['Thresh_Opt'] & (df['Sim'] > sim) & (df.Org != df.Opt)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb830f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(df):\n",
    "    calc_success(df)\n",
    "    cols = ['Assay','Org','Success']\n",
    "    df=df[cols].groupby(['Assay','Org']).agg(subtask_agg)\n",
    "    df = df.reindex(index=index, fill_value=False)\n",
    "    return df\n",
    "\n",
    "def calc_summary(df):\n",
    "    return df.groupby(['Assay']).agg('mean')\n",
    "\n",
    "def calc_all(main_df, **kwargs):\n",
    "    kwargs = dict(main=main_df, **kwargs)\n",
    "    sums = {}\n",
    "    ps = {}\n",
    "    for name,df in kwargs.items():\n",
    "        df = prepare(df)\n",
    "        kwargs[name] = df\n",
    "        sums[name] = calc_summary(df)['Success']\n",
    "    scores = pd.concat(sums.values(), axis=1, keys=sums.keys())\n",
    "    # return scores\n",
    "    for assay, score_row in scores.iterrows():\n",
    "        max_idx = score_row.idxmax()\n",
    "        assay_p = []\n",
    "        for name,df in kwargs.items():\n",
    "            assay_p.append(calc_pvalue(kwargs[max_idx], df,assay=assay)['pvalue'][assay])\n",
    "        ps[assay]=assay_p\n",
    "    \n",
    "    pss = pd.DataFrame.from_dict(ps, orient='index', columns=kwargs.keys())\n",
    "    return scores, pss\n",
    "\n",
    "def calc_div_nov_succ(seen_smls, **dfs):\n",
    "    diversity = []\n",
    "    novelty = []\n",
    "    succs = []\n",
    "    for name, df in dfs.items():\n",
    "        novelty.append((~df['Opt'].isin(seen_smls)).mean())\n",
    "        diversity.append((1-df['Sim']).mean())\n",
    "        succs.append(prepare(df)['Success'].mean())\n",
    "    data = {'metric': ['diversity','novelty','success']}\n",
    "    for i, name in enumerate(dfs.keys()):\n",
    "        data[name] = [diversity[i], novelty[i],succs[i]]\n",
    "    div_nov_df = pd.DataFrame(data).set_index('metric')\n",
    "    return div_nov_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4_gen = get_generated('experiments/modolo_margin_v2/2025_06_24_23_34/test_results/ModoloLightning/%s')\n",
    "v4_df = score_mols_df(v4_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calc_all(v4_df)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FSdock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
